<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="icon" type="image/png" href="assets/icon.png" />
  <title>ECHO</title>
  <meta name="description" content="The newest in-the-wild image generation benchmark">
  <meta name="keywords" content="unified model, image generation benchmark, native multimodal model">

  <meta property="og:type" content="website">
  <meta property="og:title" content="ECHO">

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-H80KJ6TNDZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-H80KJ6TNDZ');
  </script>

  <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
  <!-- popper used for tooltips -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <!-- bootstrap styling used for carousel -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/custom.css">

  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;600;700&display=swap" rel="stylesheet"/>
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:wght@300;400;600;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Comic+Neue:wght@300;400;600;700&display=swap" rel="stylesheet">
  
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/interactions.js"></script>

  <style>
    body {
        font-family: 'Roboto', sans-serif;
    }
  </style>

</head>
<body>

<section class="hero">
  <div class="hero-body" style="padding: 5rem 1.5rem 0.5rem 1.5rem;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h1 class="title is-size-2 publication-title section-header" style="padding-bottom: 0.3em;">
    			  Constantly Improving Image Models<br> 
    			  Need Constantly Improving Benchmarks
    		  </h1>
          <div class="is-size-5 publication-authors">
            The newest in-the-wild image generation benchmark
          </div>
          <div class="fullpage-line" style="margin-bottom: 1.5em;"></div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://jiaxin.ge" target="_blank">Jiaxin Ge</a><sup>*</sup>
            </span>
            <span class="author-block">
              <a href="https://graceluo.net" target="_blank">Grace Luo</a><sup>*</sup>
            </span>
            <br>
            <span class="author-block">
              <a href="https://kyunnilee.github.io" target="_blank">
              Heekyung Lee</a>
            </span>
            <span class="author-block">
              Nishant Malpani
            </span>
            <span class="author-block">
              <a href="https://tonylian.com" target="_blank">
              Long Lian</a>
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~xdwang" target="_blank">
              XuDong Wang</a>
            </span>
            <br>
            <span class="author-block">
              <a href="https://holynski.org/" target="_blank">Aleksander Holynski</a>
            </span> 
            <span class="author-block">
              <a href="http://people.eecs.berkeley.edu/~trevor" target="_blank">Trevor Darrell</a>
            </span>  
            <span class="author-block">
              <a href="https://www.sewonmin.com" target="_blank">Sewon Min</a>
            </span>  
            <span class="author-block">
              <a href="https://dchan.cc" target="_blank">David M. Chan</a>
          </div>
          <div class="is-size-7 publication-authors" style="margin-top: 20px;">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>
          <br>
          <div class="is-size-6 publication-authors">
            <span class="author-block">UC Berkeley</span>
          </div>
          <br>
          <div>
            <div class="publication-links">
              <span class="link-block">
                <a href="" target="_blank" class="external-link button is-normal is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/datasets/echo-bench/echo-bench" target="_blank" class="external-link button is-normal is-dark" style="box-shadow: 3px 3px 0px black;">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/para-lost/ECHO" target="_blank" class="external-link button is-normal is-dark" style="box-shadow: 3px 3px 0px black;">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
            <div class="fullpage-line" style="margin-top: 1.5em;"></div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Abstract with TOC -->
<section class="section">
  <div class="container" style="max-width: 1280px;">
    <div class="columns">
      <div class="column is-one-fifth">
        <aside class="menu">
          <b style="padding: 0.75em">Contents</b>
          <ul class="menu-list" style="margin-top: 0.5em">
            <li><a href="#method">Method</a></li>
            <li><a href="#dataset-comparison">Dataset Comparison</a></li>
            <li><a href="#community-feedback">Community Feedback</a></li>
            <li><a href="#overall-evaluation">How Do Models Compare?</a></li>
            <li><a href="#specialized-metrics">Specialized Metrics</a></li>
            <li><a href="#post-volume">Post Volume</a></li>
          </ul>
        </aside>
      </div>
      <div class="column is-four-fifths" style="max-width: 768px;">
        <h2 id="abstract" class="title is-3 section-header">Abstract</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
          Recent advances in image generation, often driven by proprietary systems like GPT-4o Image Gen, regularly introduce new capabilities that reshape how users interact with these models. Existing benchmarks often lag behind and fail to capture these emerging use cases, leaving a gap between community perceptions of progress and formal evaluation. To address this, we present <span class="echo">ECHO</span>, a framework for constructing benchmarks directly from real-world evidence of model use: social media posts that showcase novel prompts and qualitative user judgments. Applying this framework to GPT-4o Image Gen, we construct a dataset of over 35,000 prompts curated from such posts. Our analysis shows that <span class="echo">ECHO</span> (1) discovers creative and complex tasks absent from existing benchmarks, such as re-rendering product labels across languages or generating receipts with specified totals, (2) more clearly distinguishes state-of-the-art models from alternatives, and (3) surfaces community feedback that we use to inform the design of metrics for model quality (e.g., measuring observed shifts in color, identity, and structure).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<a href="#top" class="back-to-top" title="Back to top">
  <svg xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="#ccc" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
    <line x1="12" y1="19" x2="12" y2="7"></line>
    <polyline points="5 13 12 7 19 13"></polyline>
    <line x1="5" y1="4" x2="19" y2="4"></line>
  </svg>
</a>
<!-- Abstract with TOC -->

<!-- Gallery -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 section-header">Explore the Dataset</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
            <span class="echo">ECHO</span> distills collective discussion about a new generative model into a structured benchmark.
            As a case study, we apply <span class="echo">ECHO</span> to GPT-4o Image Gen on Twitter/X. 
            Here, we display a few diverse and novel tasks surfaced by <span class="echo">ECHO</span>.
          </p>
        </div>
        <div id="gallery" class="carousel slide" data-ride="carousel">
          <div class="carousel-inner"></div>
          <ol class="carousel-indicators"></ol>
          <a class="carousel-control-prev" href="#gallery" role="button" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
            <span class="sr-only">Previous</span>
          </a>
          <a class="carousel-control-next" href="#gallery" role="button" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
            <span class="sr-only">Next</span>
          </a>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Gallery -->

<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 id="method" class="title is-3 section-header">Method</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
            To collect these examples, we develop a framework called <span class="echo">ECHO</span>: <u>E</u>xtracting <u>C</u>ommunity <u>H</u>atched <u>O</u>bservations. We design this framework to address a number of challenges inherent to social media.
            Click through the carousel below to learn more about each step.
          </p>
        </div>
        <div id="method-gallery" class="carousel slide" data-ride="carousel" data-interval="10000">
          <div class="carousel-inner">
            <div class="carousel-item active">
              <div class="content has-text-left gallery-box">
                <h2 class="title is-5">1. Collect large volume of relevant posts</h2>
                <p>Large-scale collection is bottlenecked by a volume-relevance tradeoff. When querying with broader keywords, the average post relevance goes down, and with narrower ones, the available post pool is quickly exhausted. We therefore implement a two-stage pipeline, where we first query for a large volume of posts then use an LLM to filter irrelevant ones.</p>
                <img style="max-width: 450px;" src="assets/approach_1.jpg" class="gallery-image">
              </div>
            </div>
            <div class="carousel-item">
              <div class="content has-text-left gallery-box">
                <h2 class="title is-5">2. Reconstruct context with post trees</h2>
                <p>Posts can be context dependent. For example, a user <a href="https://x.com/Cliffinkent/status/1928017830538080303" target="_blank">may write "prompt below" in the first post</a> then include the actual prompt text in a reply. To extract self-contained prompts, our framework attempts to collect as much of the reply tree as possible, then use this full context when processing posts into samples.</p>
                <img style="max-width: 450px;" src="assets/approach_2.jpg" class="gallery-image">
              </div>
            </div>
            <div class="carousel-item">
              <div class="content has-text-left gallery-box">
                <h2 class="title is-5">3. Process multimodal data in non-standard formats</h2>
                <p>Useful data exists in non-standard formats. The output image could be the <a href="https://x.com/Ror_Fly/status/1906362188429705598" target="_blank">first</a> or the <a href="https://x.com/sanchoyai/status/1923655623239139422" target="_blank">last</a> in a series of images, the prompt may be written in an <a href="https://x.com/umesh_ai/status/1923628366717997323" target="_blank">incomplete fill-in-the-blank format</a>, or data may be <a href="https://x.com/AdamBartas/status/1906753444905623682" target="_blank">embedded in a screenshot</a>. We process these cases with a VLM, which is responsible for classifying input vs. output images, filling in blanks, or parsing screenshots.</p>
                <div class="sticky-scroll" style="max-width: 450px; height: 150px; overflow-y: auto; margin: 0 auto;">
                  <img src="assets/approach_3.jpg" class="gallery-image">
                </div>
              </div>
            </div>
            <div class="carousel-item">
              <div class="content has-text-left gallery-box">
                <h2 class="title is-5">4. Finalize samples</h2>
                <p> Data quality varies widely. A user may provide <a href="https://x.com/freepik/status/1919780241943494704" target="_blank">more general commentary</a> or <a href="https://x.com/aziz4ai/status/1923980161713639692" target="_blank">exactly document their input prompt</a>. We separate prompts into two groups: moderate-quality ones suited for analysis, and high-quality ones appropriate for benchmarking.</p>
                <img style="max-width: 400px;" src="assets/approach_4.jpg" class="gallery-image">
              </div>
            </div>
          </div>
          <ol class="carousel-indicators">
            <li data-target="#method-gallery" data-slide-to="0" class="active"></li>
            <li data-target="#method-gallery" data-slide-to="1"></li>
            <li data-target="#method-gallery" data-slide-to="2"></li>
            <li data-target="#method-gallery" data-slide-to="3"></li>
          </ol>
          <a class="carousel-control-prev" href="#method-gallery" role="button" data-slide="prev">
            <span class="carousel-control-prev-icon" aria-hidden="true"></span>
            <span class="sr-only">Previous</span>
          </a>
          <a class="carousel-control-next" href="#method-gallery" role="button" data-slide="next">
            <span class="carousel-control-next-icon" aria-hidden="true"></span>
            <span class="sr-only">Next</span>
          </a>
        </div>
      </div>  
    </div>
  </div>
</section>
<!-- Method -->

<!-- Dataset Comparison -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 id="dataset-comparison" class="title is-3 section-header">Dataset Comparison</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
            Using the <span class="echo">ECHO</span> framework, we are able to collect prompts that are highly diverse and closer to natural user language.
            We show a <a href="https://lvdmaaten.github.io/tsne" target="_blank">t-SNE</a> visualization comparing the prompts from <span class="echo">ECHO</span> and prior text-to-image or image-to-image datasets. 
            Prior datasets look quite different; they often contain keyword lists tailored towards Stable Diffusion (<a href="https://arxiv.org/abs/2305.01569" target="_blank">Pick-a-Pic</a>, <a href="https://arxiv.org/abs/2304.05977" target="_blank">ImageReward</a>), templated text with fixed structures (<a href="https://arxiv.org/abs/2310.11513" target="_blank">GenEval</a>), or limited task types characterized by a small set of first bigrams (<a href="https://step1x-edit.github.io" target="_blank">GEdit</a>, <a href="https://osu-nlp-group.github.io/MagicBrush" target="_blank">MagicBrush</a>, <a href="https://github.com/timothybrooks/instruct-pix2pix" target="_blank">InstructPix2Pix</a>). Conversely, prompts from <span class="echo">ECHO</span> are longer and specify a broader range of tasks. Hover over each point to view the prompts, and click on each legend item to toggle each dataset.
          </p>
        </div>
        <div id="select-dataset-comparison" class="select-row">
          <span>
            Split:
            <select onchange="changePlotly('dataset-comparison')">
              <option value="text_to_image">Text-to-Image</option>
              <option value="image_to_image">Image-to-Image</option>
            </select>
          </span>
        </div>
        <div style="padding-top: 5px; color: #C7C7C7;">
          <img style="width: 20px; margin-top: -3px;" src="assets/info.svg" />
          Select from the dropdown above then hover over the plot
        </div>
        <div style="max-width:430px;overflow:auto; margin:0 auto;">
          <iframe id="dataset-comparison-frame" src="assets/dataset_comparison/text_to_image.html" style="border:none; width:100%; height:430px;">
          </iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Dataset Comparison -->

<!-- Community Feedback -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 id="community-feedback" class="title is-3 section-header">Community Feedback</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
            <span class="echo">ECHO</span> not only extracts prompts but also community feedback, or qualitative comments on generated outputs. We use an LLM to annotate
            whether each comment discusses a success or failure, and extract keywords. We visualize the keywords of all failures in the following word cloud.
            Select from the dropdown to explore some of the suggested keywords,
            which highlight the attributes that users are sensitive to (e.g., "identity", "aspect ratio", "proportions", text accuracy", "color balance", "coherency"). You can also click the word cloud to explore examples.
          </p>
        </div>
        <div style="padding-top: 5px; color: #C7C7C7;">
          <img style="width: 20px; margin-top: -3px;" src="assets/info.svg" />
          Select from keyword dropdown or click the word cloud
        </div>
        <br>
        <div style="max-width:1000px; overflow:auto; margin:0 auto;">
          <iframe id="post-volume-frame" src="assets/community_feedback.html" style="border:none; width:100%;">
          </iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Community Feedback -->

<!-- Overall Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 id="overall-evaluation" class="title is-3 section-header">How Do Models Compare?</h2>
        <div class="header-line"></div> 
        <div style="display: flex; justify-content: center;">
          <img id="" src="" style="max-width: 70%;">
        </div>
        <div class="content has-text-left">
          <p>
            We then compare the performance of a range of models on <span class="echo">ECHO</span> samples, for both image-to-image and text-to-image tasks. We evaluate both open-source (<a href="https://arxiv.org/abs/2407.06135" target="_blank">Anole<a>, <a href="https://arxiv.org/abs/2505.14683" target="_blank">Bagel<a>) and closed-source (<a href="https://openai.com/index/introducing-4o-image-generation" target="_blank">4o Image Gen</a>, <a href="https://developers.googleblog.com/en/introducing-gemini-2-5-flash-image" target="_blank">Nano Banana</a>, <a href="https://developers.googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation" target="_blank">Gemini 2.0 Flash<a>) unified models. We also evaluate the most naive implementation of a "unified model," GPT4o chained to DALLE-3 (<a href="https://openai.com/index/dall-e-3" target="_blank">LLM+Diffusion<a>). Finally, we compare a state-of-the-art image editing model (<a href="https://arxiv.org/abs/2506.15742" target="_blank">Flux Kontext</a>).
            We report the win rate computed via an ensemble of VLM-as-a-judge, following the "single answer grading" setup from <a href="https://arxiv.org/abs/2306.05685" target="_blank">MT-Bench</a>.
          </p>
        </div>
        <div class="columns">
          <div class="column">
            <figure class="image">
              <img src="assets/winrate_1.jpg" />
            </figure>
          </div>
          <div class="column">
            <figure class="image">
              <img src="assets/winrate_2.jpg" />
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Overall Evaluation -->

<!-- Specialized Metrics -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 id="specialized-metrics" class="title is-3 section-header">Specialized Metrics from Community Feedback</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
          We also design several specialized automated metrics, inspired by the failure categories <a href="#community-feedback">discovered from the community feedback</a>: color shift magnitude, face identity similarity, structure distance, and text rendering accuracy. For each metric, we use an LLM to classify samples where each metric is applicable, and compute the metric over these samples. While 4o Image Gen may be proficient in overall instruction following, users also notice that it exhibits large shifts in color and face identity, which we validate and quantify. Click the dropdown to take a look at each metric, and toggle the slider to see example model outputs given the same input!
          </p>
        </div>
        <div id="select-specialized" class="select-row">
          <span>
            Metric:
            <select onchange="populateModelComparison()">
              <option value="color_shift">Color Shift Magnitude</option>
              <option value="face_identity">Face Identity Similarity</option>
              <option value="structure_distance">Structure Distance</option>
              <option value="text_rendering">Text Rendering Accuracy</option>
            </select>
          </span>
        </div>
        <div style="padding-top: 5px; color: #C7C7C7;">
          <img style="width: 20px; margin-top: -3px;" src="assets/info.svg" />
          Select from the dropdown and toggle the slider
        </div>
        <div style="max-width: 400px; margin: 0 auto;"><img id="specialized-chart" src="assets/specialized_metrics/color_shift.jpg"/></div>
        <div id="specialized-comparison-grid"></div>
      </div>
    </div>
  </div>
</section>
<!-- Specialized Metrics -->

<!-- Post Volume -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 id="post-volume" class="title is-3 section-header">Post Volume</h2>
        <div class="header-line"></div> 
        <div class="content has-text-left">
          <p>
            Since the <span class="echo">ECHO</span> framework operates on a large-scale, it can give insight into activity surrounding a model of interest.
            We plot the timestamps of posts, after relevance and quality filtering, bucketed by day.
            Spikes in activity (highlighted in yellow) often align with real-world events, including: the day after the 4o Image Gen model release (<a href="https://openai.com/index/introducing-4o-image-generation" target="_blank">Mar 26</a>), the day of the o3 model release (<a href="https://openai.com/index/introducing-o3-and-o4-mini" target="_blank">Apr 16</a>), and the day after the 4o Image Gen API release (<a href="https://openai.com/index/image-generation-api" target="_blank">Apr 24</a>).
          </p>
        </div>
        <div style="padding-top: 5px; color: #C7C7C7;">
          <img style="width: 20px; margin-top: -3px;" src="assets/info.svg" />
          Hover over the plot to see counts
        </div>
        <br>
        <div style="max-width:630px; overflow:auto; margin:0 auto;">
          <iframe id="post-volume-frame" src="assets/post_volume.html" style="border:none; width:100%; height:330px;">
          </iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- Post Volume -->

<!-- Acknowledgements and Citation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4 section-header plain-header">Acknowledgements</h2>
        <p>
          We thank Stephanie Fu, Michelle Li, and Alexander Pan for their helpful feedback.
          We also thank the folks at Stochastic Labs for previewing early prototypes of this work. Finally, we extend a special thank you to Lisa Dunlap for entertaining many extensive discussions on evaluations.
        </p>
        <br>
        <h2 class="title is-4 section-header plain-header">About This Website</h2>
        <p>
          <a href="https://graceluo.net" target="_blank">Grace</a> really enjoys web design, so here's a short epilogue discussing all the bells and whistles.
          It turns out, it's possible to make the <a href="https://nerfies.github.io">Nerfies</a> template feel more "bloggy" simply by aligning text to the left instead of center, then moving paragraphs before figures instead of after. We then used <a href="https://plotly.com/python" target="_blank">Plotly</a> for all the interactive visualizations. Final touches include: a loading screen for the visualizations, a "back to top" button anchored on the bottom right, and a lot of breakpoint magic for mobile viewing. Enjoy!
        </p>
        <br>
        <h2 class="title is-4 section-header plain-header">BibTeX</h2>
<pre><code>@article{ge2025echo,
  title={Constantly Improving Image Models Need Constantly Improving Benchmarks},
  author={Jiaxin Ge, Grace Luo, Heekyung Lee, Nishant Malpani, Long Lian, XuDong Wang, Aleksander Holynski, Trevor Darrell, Sewon Min, David M. Chan},
  journal={arXiv},
  year={2025}
}
</code></pre>
      </div>
    </div>
  </div>
</section>
<!-- Acknowledgements and Citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            The website is modified from the <a href="https://dual-process.github.io">Dual-Process Image Generation</a> project page, which is based on <a href="https://nerfies.github.io">Nerfies</a>. We adapted it taking inspiration from <a href="https://flowreinforce.github.io">FPO</a>, which is spiritually similar to the <a href="https://distill.pub/guide"> Distill Template</a>. Feel free to use this website's <a href="https://github.com/echo-bench/echo-bench.github.io">source code</a> as a template and <a href="https://echo-bench.github.io">cite our project</a>!
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
